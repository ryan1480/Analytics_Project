{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 1 – Load and Inspect the Data\n",
    "\n",
    "In this step, we load the cleaned dataset `Fraud_Cleaned.csv` into a pandas DataFrame and perform an initial inspection. \n",
    "We check for missing values, confirm data types, and analyze the class distribution of the target variable `fraud`.\n",
    "This step ensures the dataset is clean and structurally ready for training and evaluation.\n",
    "\n",
    "### Goals of this step:\n",
    "- Ensure no missing values are present\n",
    "- Verify that all feature columns are properly formatted\n",
    "- Analyze class imbalance in the target column\n",
    "- Review basic feature distributions and dataset size"
   ],
   "id": "c6fe2f5c7d03742"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T13:56:00.201977Z",
     "start_time": "2025-05-23T13:55:59.514390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Schritt 2: CSV-Datei einlesen (Pfad ggf. anpassen)\n",
    "df = pd.read_csv(\"../Data/Fraud_Cleaned.csv\")\n",
    "\n",
    "# Schritt 3: Features und Zielvariable definieren\n",
    "X = df.drop(\"fraud\", axis=1)  # Alle Spalten außer der Zielvariable\n",
    "y = df[\"fraud\"]               # Zielvariable (0 = kein Betrug, 1 = Betrug)\n",
    "\n",
    "# Schritt 4: Aufteilen in Trainings-, Validierungs- und Testset (70/15/15) mit stratifizierter Verteilung\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Optional: Shapes ausgeben zur Kontrolle\n",
    "print(f\"Trainingsdaten: {X_train.shape}\")\n",
    "print(f\"Validierungsdaten: {X_val.shape}\")\n",
    "print(f\"Testdaten: {X_test.shape}\")"
   ],
   "id": "1009e83bf5f1dcb8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainingsdaten: (348635, 9)\n",
      "Validierungsdaten: (74707, 9)\n",
      "Testdaten: (74708, 9)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 2 – Data Splitting (Train, Validation, Test)\n",
    "\n",
    "To prepare for model training and evaluation, we split the dataset into three parts:\n",
    "\n",
    "- **70%** Training Set: Used to fit the model\n",
    "- **15%** Validation Set: Used for threshold tuning and cost evaluation\n",
    "- **15%** Test Set: Used for final model evaluation\n",
    "\n",
    "We use stratified sampling to ensure that the distribution of the target class `fraud` remains consistent across all splits. This is especially important given the strong class imbalance (only ~4.8% of fraud cases)."
   ],
   "id": "6b9c6f09f793775f"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-23T13:56:03.188355Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    }
   ],
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "import numpy as np\n",
    "\n",
    "# Random Forest Modell\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced', n_jobs=-1)\n",
    "\n",
    "# Hyperparameter-Suchraum definieren\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV Setup\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,\n",
    "    cv=5,\n",
    "    scoring='f1',  # F1-Score als initiale Metrik\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Modelltraining mit Hyperparameter-Tuning\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Bestes Modell sichern\n",
    "best_rf = random_search.best_estimator_\n",
    "\n",
    "# Beste Parameter anzeigen\n",
    "best_params = random_search.best_params_\n",
    "best_params"
   ],
   "id": "171213527f05e2f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 3 – Threshold Tuning Based on Cost\n",
    "\n",
    "After training the classification model, we do not simply use the default threshold of 0.5.  \n",
    "Instead, we perform **cost-based threshold tuning** on the validation set, considering the business impact:\n",
    "\n",
    "- A **False Positive (FP)** leads to an unnecessary control of an honest customer, which is expensive and damaging\n",
    "- A **False Negative (FN)** means a fraudster goes undetected, which also has cost implications\n",
    "- In our case: **FP is 5× more costly than FN**\n",
    "\n",
    "We calculate the total cost for various thresholds (from 0.01 to 0.99) and select the one with the lowest overall cost."
   ],
   "id": "b746b0b28e40c907"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T13:30:12.645718Z",
     "start_time": "2025-05-23T13:30:11.812401Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimaler Threshold: 0.64, Minimale Kosten: 491\n"
     ]
    }
   ],
   "execution_count": 7,
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "y_val_proba = best_rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "thresholds = np.arange(0.01, 1.00, 0.01)\n",
    "costs = []\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred = (y_val_proba >= t).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
    "    cost = fp * 5 + fn\n",
    "    costs.append(cost)\n",
    "\n",
    "optimal_threshold = thresholds[np.argmin(costs)]\n",
    "print(f\"Optimaler Threshold: {optimal_threshold:.2f}, Minimale Kosten: {min(costs)}\")"
   ],
   "id": "ab2f49a6546b55dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 4 – Final Model Evaluation on Test Set\n",
    "\n",
    "Now that we have identified the optimal threshold based on cost minimization, we evaluate the model performance on the **unseen test set**.\n",
    "\n",
    "We use the trained model and apply the optimal threshold to classify test instances. We then calculate:\n",
    "- The **confusion matrix**\n",
    "- The **total cost**: (False Positives × 5 + False Negatives)\n",
    "- **Precision**, **Recall**, and **F1-score** for the minority class (`fraud = 1`)\n",
    "\n",
    "This gives us a realistic and fair assessment of how well the model would perform in production."
   ],
   "id": "e36ee5c929eb624e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T13:33:41.917579Z",
     "start_time": "2025-05-23T13:33:41.637203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Schritt 4: Vorhersage auf dem Testset\n",
    "y_test_proba = best_rf.predict_proba(X_test)[:, 1]\n",
    "y_test_pred = (y_test_proba >= optimal_threshold).astype(int)\n",
    "\n",
    "# Confusion Matrix und finale Kosten berechnen\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_test_pred).ravel()\n",
    "final_cost = fp * 5 + fn\n",
    "print(f\"Finale Kosten (FP×5 + FN): {final_cost}\")\n",
    "\n",
    "# Klassifikationsbericht (nur Klasse 1 wichtig)\n",
    "report = classification_report(y_test, y_test_pred)\n",
    "print(\"Klassifikationsbericht:\")\n",
    "print(report)"
   ],
   "id": "8cc6636bdc16f46d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finale Kosten (FP×5 + FN): 567\n",
      "Klassifikationsbericht:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     71149\n",
      "           1       0.99      0.91      0.95      3559\n",
      "\n",
      "    accuracy                           1.00     74708\n",
      "   macro avg       0.99      0.95      0.97     74708\n",
      "weighted avg       0.99      1.00      0.99     74708\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "7aca51fdb4b431b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b50e212e704f943"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay\n",
    "\n",
    "# 1. Kostenkurve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, costs, label=\"Gesamtkosten (FP×5 + FN)\")\n",
    "plt.axvline(x=optimal_threshold, color='red', linestyle='--', label=f\"Optimaler Threshold = {optimal_threshold:.2f}\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Kosten\")\n",
    "plt.title(\"Kostenbasierte Threshold-Optimierung (Validation Set)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Confusion Matrix auf dem Testset\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "            xticklabels=[\"No Fraud\", \"Fraud\"], yticklabels=[\"No Fraud\", \"Fraud\"])\n",
    "plt.xlabel(\"Vorhergesagt\")\n",
    "plt.ylabel(\"Tatsächlich\")\n",
    "plt.title(\"Confusion Matrix (Test Set)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. ROC-Kurve\n",
    "RocCurveDisplay.from_estimator(best_rf, X_test, y_test)\n",
    "plt.title(\"ROC-Kurve (Test Set)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Precision-Recall-Kurve\n",
    "PrecisionRecallDisplay.from_estimator(best_rf, X_test, y_test)\n",
    "plt.title(\"Precision-Recall-Kurve (Test Set)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "40f7da201de18890",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
