{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb904670-247f-4718-904c-13197a46ee3c",
   "metadata": {},
   "source": [
    "# Library Imports & Environment Setup\n",
    "In this cell, we import all necessary Python libraries for our analysis. pandas and numpy handle data manipulation and numerical computations, while matplotlib and seaborn support data visualization. We also bring in modules from scikit-learn for splitting the dataset, building a Logistic Regression model, and evaluating its performance using metrics like accuracy, confusion matrix, ROC curve, and AUC. Finally, joblib is imported for saving and loading trained models.\n",
    "\n",
    "This setup ensures our environment is fully equipped for data processing, modeling, and evaluation tasks."
   ]
  },
  {
   "cell_type": "code",
   "id": "6af849ee-5a41-4c71-9546-0010984eb96d",
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "import pandas as pd                  # For data manipulation\n",
    "import numpy as np                   # For numerical operations\n",
    "import matplotlib.pyplot as plt      # For plotting\n",
    "import seaborn as sns                # For visualizations"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7d43eebf-d451-4968-b7be-398d0e216dd2",
   "metadata": {},
   "source": [
    "# Data Loading & Initial Exploration\n",
    "In this section, we load the cleaned fraud detection dataset using pandas. After importing the data, we perform a preliminary examination by displaying its structure (info()), the first few rows (head()), and basic statistical summaries (describe()). This helps us understand the dataset’s shape, types of variables, and general distribution before moving into deeper analysis."
   ]
  },
  {
   "cell_type": "code",
   "id": "daef5660-3483-4063-a47a-c2f51bddd1fc",
   "metadata": {},
   "source": [
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"../Data/Fraud_Cleaned.csv\")  # Ensure the correct file path\n",
    "\n",
    "# Display basic info\n",
    "df.info()\n",
    "df.head()\n",
    "df.describe()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4bb46301-4f53-4085-b5c3-040937c5faf6",
   "metadata": {},
   "source": [
    "# Data Exploration \n",
    "This section focuses on identifying and addressing potential data quality issues. We begin by checking for missing values and duplicated rows within the dataset. These steps are essential to ensure data integrity and reliability before proceeding to model training and analysis."
   ]
  },
  {
   "cell_type": "code",
   "id": "33a13dd0-68d1-43bd-b5fc-e248f13661b1",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing Values:\\n\", df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(\"\\nDuplicate Rows:\", df.duplicated().sum())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f79aa01b-8588-43a1-b172-5ea446b3a538",
   "metadata": {},
   "source": [
    "# Class Distribution\n",
    "We visualize the distribution of fraud vs. non-fraud cases and highlight the fraud class for clarity. Class percentages are also printed to check for imbalance, which is important for model performance."
   ]
  },
  {
   "cell_type": "code",
   "id": "1b16649a-0e5b-44a3-92ef-cc02821a25b8",
   "metadata": {},
   "source": [
    "# Visualize class distribution with custom color for fraud = 1\n",
    "ax = sns.countplot(x='fraud', data=df)\n",
    "ax.patches[1].set_color('#DD8452')  # Set fraud = 1 bar to orange\n",
    "plt.title('Class Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Print class percentages\n",
    "print(\"Class distribution (%):\")\n",
    "print(df['fraud'].value_counts(normalize=True) * 100)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ca75e3e8-28d7-4b60-aa4c-a6cace35dd51",
   "metadata": {},
   "source": [
    "The distribution of the target variable fraud is highly imbalanced. The vast majority of transactions are labeled as non-fraudulent (fraud = 0), while only a small fraction are labeled as fraudulent (fraud = 1). Specifically, approximately 95% of the records are non-fraudulent, with only 5% labeled as fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2806ac-3e7b-405d-9095-02aa430b1f12",
   "metadata": {},
   "source": [
    "# Feature Distribution by Fraud Status\n",
    "We examine how each feature is distributed across fraud and non-fraud cases. Continuous features are shown with KDE plots, while categorical ones use stacked histograms. This helps identify patterns and potential predictors of fraud."
   ]
  },
  {
   "cell_type": "code",
   "id": "a98fdae6-a716-41c6-b916-e2281bee7b15",
   "metadata": {},
   "source": [
    "\n",
    "features_to_plot = df.columns.drop(\"fraud\")  # exclude target\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "for i, feature in enumerate(features_to_plot, 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    \n",
    "    unique_vals = df[feature].nunique()\n",
    "    \n",
    "    # Use KDE only for continuous features\n",
    "    if unique_vals > 20:  # arbitrary threshold;\n",
    "        sns.histplot(data=df, x=feature, hue=\"fraud\", kde=True, bins=30, element=\"step\")\n",
    "    else:\n",
    "        sns.histplot(data=df, x=feature, hue=\"fraud\", kde=False, bins=unique_vals, multiple=\"stack\", shrink=0.8)\n",
    "    \n",
    "    plt.title(f\"{feature} by Fraud Status\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0512bc58-a1ac-42bb-9b7f-cc0136372223",
   "metadata": {},
   "source": [
    "The feature trustLevel shows that most non-fraudulent transactions occur at higher trust levels (3–6), while fraud is concentrated at the lowest levels (1 and 2), indicating trust is a strong fraud indicator. totalScanTimeInSeconds and grandTotal display wide distributions with slight elevation in fraud at longer scan durations and higher values, though the separation is not visually distinct. lineItemVoids, scansWithoutRegistration, and quantityModification all show more fraud cases at higher values, supporting their relevance as behavioral indicators of manipulation. In contrast, scannedLineItemsPerSecond and valuePerSecond are extremely skewed, with very few high-value outliers — potentially useful after transformation. Finally, lineItemVoidsPerPosition displays a clear concentration of fraud at higher values, confirming its strong predictive signal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80259ad9-0a40-4c44-834c-9d3064e55346",
   "metadata": {},
   "source": [
    "# Fraud Breakdown by Low-Cardinality Features\n",
    "This section analyzes categorical features with few unique values to see how fraud rates vary across their categories. It provides a detailed breakdown of fraud and non-fraud counts and percentages for each feature value, helping reveal patterns and potential risk indicators."
   ]
  },
  {
   "cell_type": "code",
   "id": "099d3eb0-6bb8-46ec-b1ff-862a122187ab",
   "metadata": {},
   "source": [
    "\n",
    "# Define the target column\n",
    "target_col = \"fraud\"\n",
    "\n",
    "# Define what qualifies as \"low-cardinality\"\n",
    "max_unique = 12\n",
    "candidate_features = [col for col in df.columns if col != target_col and df[col].nunique() <= max_unique]\n",
    "\n",
    "# Loop over each low-cardinality feature\n",
    "for feature in candidate_features:\n",
    "    summary_rows = []\n",
    "\n",
    "    for val in sorted(df[feature].unique()):\n",
    "        total_count = df[df[feature] == val].shape[0]\n",
    "        count_yes = df[(df[feature] == val) & (df[target_col] == 1)].shape[0]\n",
    "        count_no = df[(df[feature] == val) & (df[target_col] == 0)].shape[0]\n",
    "        pct_yes = (count_yes / total_count) * 100 if total_count > 0 else 0\n",
    "        pct_no = (count_no / total_count) * 100 if total_count > 0 else 0\n",
    "\n",
    "        summary_rows.append({\n",
    "            \"Value\": val,\n",
    "            \"Total Count\": total_count,\n",
    "            \"Fraud Count (Yes)\": count_yes,\n",
    "            \"Non-Fraud Count (No)\": count_no,\n",
    "            \"% Fraud (Yes)\": round(pct_yes, 2),\n",
    "            \"% Non-Fraud (No)\": round(pct_no, 2),\n",
    "        })\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "    print(f\"\\n=== Feature: {feature} ===\")\n",
    "    display(summary_df)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3cb42011-49c5-4b77-bacc-4e1359d76dd9",
   "metadata": {},
   "source": [
    "## trustLevel\n",
    "The distribution clearly shows that fraud occurs only at the lowest trust levels (1 and 2), with 22.41% and 6.26% fraud rates respectively. From trust level 3 onward, no fraud is recorded, and all transactions are non-fraudulent. This makes trustLevel one of the strongest features in distinguishing between fraudulent and non-fraudulent behavior — the lower the trust level, the higher the risk.\n",
    "\n",
    "## lineItemVoids\n",
    "Fraud percentage increases gradually as the number of voided items increases. For example, fraud rates grow from ~2.6% at 1 void to 6.68% at 11 voids. This steady climb suggests that frequent voiding of items is a clear fraud indicator, and the model should weigh this feature accordingly.\n",
    "\n",
    "## scansWithoutRegistration\n",
    "This feature also displays a positive correlation with fraud likelihood. The fraud rate rises from 2.09% at 0 attempts to over 7% at 10 attempts. This supports the idea that customers who repeatedly scan without registering items are exhibiting suspicious behavior, making this another strong behavioral predictor of fraud.\n",
    "\n",
    "## quantityModification\n",
    "Fraud rates across different quantity modification values (0–5) are quite stable, hovering between 4.7% and 4.85%. This lack of variation suggests that quantityModification does not strongly differentiate between fraud and non-fraud cases. It may be less valuable as a standalone feature but could still support other interactions in the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652883db-c280-40bb-9e0f-be94dfc09d3a",
   "metadata": {},
   "source": [
    "# Feature Correlation Matrix\n",
    "We use a heatmap to visualize correlations between features. This helps identify multicollinearity and relationships that may influence model performance."
   ]
  },
  {
   "cell_type": "code",
   "id": "67c41429-d3c3-4e99-b401-713f0c657775",
   "metadata": {},
   "source": [
    "selected_features = [\n",
    "    \"trustLevel\", \"totalScanTimeInSeconds\", \"lineItemVoids\", \n",
    "    \"quantityModification\", \"grandTotal\", \"scannedLineItemsPerSecond\", \n",
    "    \"valuePerSecond\", \"lineItemVoidsPerPosition\", \"scansWithoutRegistration\"\n",
    "]\n",
    "\n",
    "X = df[selected_features]\n",
    "\n",
    "# Visualize feature correlation matrix\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(X.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f0ecbd2e-12af-46d6-8546-ac2dba1f99d4",
   "metadata": {},
   "source": [
    "The correlation matrix analysis confirmed that most features are weakly correlated, minimizing the risk of multicollinearity in the logistic regression model. The only notable exception is the strong correlation (0.75) between scannedLineItemsPerSecond and valuePerSecond, which aligns with domain logic. While both features are retained for now to preserve model performance, they may be revisited during feature importance or model refinement steps. Overall, the selected features appear well-suited for modeling, requiring no immediate removals or transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc72ac8-8772-4101-9136-e8a769ea8407",
   "metadata": {},
   "source": [
    "# Outlier Detection with Boxplots\n",
    "Boxplots are used to visually inspect each feature for potential outliers. This helps identify extreme values that may affect model accuracy or require preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "id": "3b19e308-66ef-4b78-9792-c75d60eaacbb",
   "metadata": {},
   "source": [
    "# Visualize outliers using boxplots\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.boxplot(data=df, orient=\"h\")\n",
    "plt.title(\"Feature Boxplot for Outlier Detection\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ffc4f64b-3f0d-4e2c-9c62-483f52deef93",
   "metadata": {},
   "source": [
    "The boxplot reveals that most features, such as trustLevel, lineItemVoids, quantityModification, and scansWithoutRegistration, are tightly distributed with limited spread and numerous low-value entries. In contrast, totalScanTimeInSeconds and grandTotal show a wider spread and contain clear outliers, with totalScanTimeInSeconds extending well beyond 1500 seconds, indicating unusually long scanning sessions. Features like valuePerSecond and scannedLineItemsPerSecond are heavily skewed and exhibit extreme outliers, suggesting the need for normalization or transformation prior to modeling. The boxplot effectively highlights which features may require special handling to mitigate the influence of outliers on model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
